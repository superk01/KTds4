import os
import json
import uuid
import streamlit as st
from dotenv import load_dotenv
from langchain_community.retrievers import AzureAISearchRetriever
from langchain_openai import AzureChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain.memory import ConversationBufferMemory
from langchain_core.tools import tool
from langchain_community.tools import TavilySearchResults
from langchain_openai import AzureOpenAIEmbeddings
from sklearn.metrics.pairwise import cosine_similarity
from langchain_core.messages import get_buffer_string

# ========== í™˜ê²½ ë³€ìˆ˜ ë° ì´ˆê¸° ì„¤ì • ==========
load_dotenv()
HISTORY_DIR = "chat_history"
os.makedirs(HISTORY_DIR, exist_ok=True)

# UUIDë¥¼ íšë“í•˜ì—¬ ì„¸ì…˜ì•„ì´ë””ë¡œ ì‚¬ìš©
if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())  # ê³ ìœ  UUID ìƒì„±

# ========== íˆìŠ¤í† ë¦¬ ê´€ë ¨ í•¨ìˆ˜ ==========
def get_history_path(user_id):
    return os.path.join(HISTORY_DIR, f"{user_id}.json")

def save_history(user_id, history):
    with open(get_history_path(user_id), "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)

def load_history(user_id):
    path = get_history_path(user_id)
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

# ========== LLM ë° í”„ë¡¬í”„íŠ¸ ==========
llm = AzureChatOpenAI(model=os.getenv("AZURE_OPENAI_LLM2"), temperature=0)

# ========== ì—°ì† ì§ˆë¬¸ íŒë‹¨ í•¨ìˆ˜ (ì„ë² ë”© ê¸°ë°˜) ==========
embeddings = AzureOpenAIEmbeddings(
    azure_deployment=os.getenv("AZURE_OPENAI_EMBEDDING"),
    openai_api_version="2024-12-01-preview",
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    api_key=os.getenv("OPENAI_API_KEY"),
)

def is_follow_up_question(previous_question: str, current_question: str, threshold: float = 0.75) -> bool:
    prev_vec = embeddings.embed_query(previous_question)
    curr_vec = embeddings.embed_query(current_question)
    sim = cosine_similarity([prev_vec], [curr_vec])[0][0]
    # st.write(f"ğŸ” previous_question ê²°ê³¼: {previous_question}")
    # st.write(f"ğŸ” current_question ê²°ê³¼: {current_question}")
    # st.write(f"ğŸ” sim ê²°ê³¼: {sim}")
    return sim >= threshold

# ========== ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± ==========
def generate_search_query(chat_history, latest_question):
    # ì´ì „ ì§ˆë¬¸ê³¼ ì˜ë¯¸ì ìœ¼ë¡œ ì—°ì†ì¸ì§€ íŒë‹¨
    recent_user_questions = [msg["content"] for msg in chat_history if msg["role"] == "user"]
    # st.write(f"ğŸ” recent_user_questions ê²°ê³¼: {recent_user_questions}")
    if len(recent_user_questions) >= 1:
        previous_question = recent_user_questions[-1]
        #st.write(f"ğŸ” previous_question ê²°ê³¼: {previous_question}")
        if not is_follow_up_question(previous_question, latest_question):
            return latest_question  # ìƒˆë¡œìš´ ì£¼ì œì´ë¯€ë¡œ ë‹¨ë… ì‚¬ìš©

    # ì „ì²´ ëŒ€í™”ë¬¸ ìš”ì•½ì„ í™œìš©
    full_context = "\n".join(f"{msg['role'].capitalize()}: {msg['content']}" for msg in chat_history[-8:])
    # memory_context = get_buffer_string(memory.chat_memory.messages[-8:])

    # ì§ˆë¬¸ + ë‹µë³€ í¬í•¨í•œ ë§¥ë½ìœ¼ë¡œ ì¿¼ë¦¬ ìƒì„±
    # combined_context = []
    # for msg in chat_history[-4:]:
    #     combined_context.append(f"{msg['role'].capitalize()}: {msg['content']}")

    prompt = ChatPromptTemplate.from_template(
        "ë‹¤ìŒì€ ì‚¬ìš©ìì™€ AIì˜ ëŒ€í™” ê¸°ë¡ì…ë‹ˆë‹¤:\n{context}\n\n"
        "í˜„ì¬ ì§ˆë¬¸: {question}\n\n"
        "ìœ„ ëŒ€í™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ ì—”ì§„ì— ì í•©í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•´ì¤˜."
    )
    chain = prompt | llm | StrOutputParser()
    return chain.invoke({"context": full_context, "question": latest_question})


# def generate_search_query(latest_question):
#     memory_context = get_buffer_string(memory.chat_memory.messages[-8:])
    
#     prompt = ChatPromptTemplate.from_template(
#         """ë‹¤ìŒì€ ì‚¬ìš©ìì™€ AIì˜ ëŒ€í™” ê¸°ë¡ì…ë‹ˆë‹¤:\n{context}\n\n
#         í˜„ì¬ ì§ˆë¬¸: {question}\n\n
#         ìœ„ ëŒ€í™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ ì—”ì§„ì— ì í•©í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•´ì¤˜."""
#     )
#     chain = prompt | llm | StrOutputParser()
#     return chain.invoke({
#         "context": memory_context,
#         "question": latest_question
#     })

def rephrase_question(latest_question):
    memory_context = get_buffer_string(memory.chat_memory.messages[-8:])
    prompt = ChatPromptTemplate.from_template(
        """ì´ì „ ëŒ€í™”:\n{context}\n\n
        í˜„ì¬ ì§ˆë¬¸: {question}\n\n
        ë¬¸ë§¥ì„ ë°˜ì˜í•´ ëª…í™•í•œ ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¿”ì¤˜."""
    )
    chain = prompt | llm | StrOutputParser()
    return chain.invoke({
        "context": memory_context,
        "question": latest_question
    })
# ========== ë„êµ¬ ì •ì˜ ==========
@tool
def web_search(query: str) -> str:
    """Search the web using Tavily."""
    tavily = TavilySearchResults(max_results=3, search_depth="advanced", include_answer=True, include_raw_content=True, include_images=True)
    # refined = rephrase_question(query)
    # search_query = generate_search_query( refined)

    # print("ì‚¬ìš©ìê°€ ì…ë ¥í•œ query:", query)
    # print("ìµœì¢… ìƒì„±ëœ search_query:", search_query)

    # st.sidebar.write(f"ì…ë ¥ëœ query: {query}")
    # st.sidebar.write(f"ìƒì„±ëœ search_query: {search_query}")

    # st.info(f"ì…ë ¥ëœ web_query: {search_query}")
    st.write(f"web_search")
    return tavily.invoke(query)

@tool
def law_search(query: str) -> str:
    """Search for Law information in PDF files."""
    retriever = AzureAISearchRetriever(content_key="chunk", top_k=3, index_name=os.getenv("AZURE_AI_SEARCH_INDEX_NAME_LAW"))
    llm_local = AzureChatOpenAI(deployment_name=os.getenv("AZURE_OPENAI_LLM1"))
    prompt = ChatPromptTemplate.from_template("""Answer the question based only on the context provided.\nContext: {context}\nQuestion: {question}""")
    chain = ({"context": retriever | format_docs, "question": RunnablePassthrough()} | prompt | llm_local | StrOutputParser())
    # refined = rephrase_question(query)
    # search_query = generate_search_query(refined)

    # print("ì‚¬ìš©ìê°€ ì…ë ¥í•œ query:", query)
    # print("ìµœì¢… ìƒì„±ëœ search_query:", search_query)

    # st.sidebar.write(f"ì…ë ¥ëœ query: {query}")
    # st.sidebar.write(f"ìƒì„±ëœ search_query: {search_query}")

    # st.info(f"ì…ë ¥ëœ law_query: {search_query}")
    st.write(f"law_search")
    return chain.invoke({"question": query})

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

prompt = ChatPromptTemplate.from_messages([
    # ("system", "ë‹¹ì‹ ì€ ì§€ëŠ¥í˜• AI Assistantì…ë‹ˆë‹¤. ì´ì „ ì§ˆë¬¸ê³¼ ë§¥ë½ì„ ë°˜ì˜í•´ PDF ë° ì›¹ ê²€ìƒ‰ ë„êµ¬ë¥¼ í†µí•´ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤."),
#    ("system", "- 'law_search': PDF ë²•ë¥  ì¸ë±ìŠ¤ ê²€ìƒ‰\n- 'web_search': ìµœì‹  ì •ë³´ë¥¼ Tavilyë¥¼ í†µí•´ ì›¹ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤."),
    # ("system", "ì„¸ë²• ì •ë³´ ê²€ìƒ‰ì€ PDF íŒŒì¼ì—ì„œ í˜¸í…” ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ê³ , ì›¹ ê²€ìƒ‰ì€ Tavilyë¥¼ í†µí•´ ì›¹ì—ì„œ ì •ë³´ë¥¼ ì°¾ìŠµë‹ˆë‹¤."),
    # ("system", "í•­ìƒ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ì‘ë‹µí•˜ë©°, ë¶ˆëª…í™•í•œ ê²½ìš° í™•ì¸ì„ ìš”ì²­í•©ë‹ˆë‹¤."),
    ("system", "ë„ˆëŠ” ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” AI Assistantì•¼. ì´ì „ ëŒ€í™” íë¦„ì„ ë°˜ì˜í•˜ê³  í•„ìš”í•˜ë©´ ë„êµ¬ë¥¼ ì‚¬ìš©í•´."),
    MessagesPlaceholder(variable_name="chat_history"),
    ("user", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
])

# ========== ì—ì´ì „íŠ¸ ë° ì‹¤í–‰ê¸° ==========
tools = [web_search, law_search]
agent = create_tool_calling_agent(llm, tools, prompt)
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory, verbose=True)

# ========== Streamlit UI ==========
st.set_page_config(page_title="LAW & Web Chat Agent", layout="centered")
st.title("AI Assistant: ë²•ë¥  & ì›¹ ê²€ìƒ‰")

# user_id = st.text_input("ì‚¬ìš©ì ì´ë¦„ ë˜ëŠ” IDë¥¼ ì…ë ¥í•˜ì„¸ìš”", key="user_id")
# if not user_id:
#     st.warning("ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•˜ë ¤ë©´ ì‚¬ìš©ì IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
#     st.stop()

session_id = st.session_state.session_id

if "chat_history" not in st.session_state:
    st.session_state.chat_history = load_history(session_id)

if st.sidebar.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
    st.session_state.chat_history = []
    save_history(session_id, [])
    st.rerun()

# ì‚¬ì´ë“œë°”ì— ëŒ€í™” íˆìŠ¤í† ë¦¬ ìš”ì•½ í‘œì‹œ
st.sidebar.markdown("## ğŸ’¬ ì´ì „ ëŒ€í™” ë³´ê¸°")
chat_pairs, current_pair = [], {}
for msg in st.session_state.chat_history:
    if msg["role"] == "user":
        if current_pair and "user" in current_pair:
            chat_pairs.append(current_pair)
        current_pair = {"user": msg["content"], "assistant": ""}
    elif msg["role"] == "assistant" and current_pair:
        current_pair["assistant"] = msg["content"]
if current_pair and "user" in current_pair:
    chat_pairs.append(current_pair)

for i, pair in enumerate(chat_pairs):
    with st.sidebar.expander(f"â“ Q{i+1}: {pair['user'][:30]}..."):
        st.markdown(f"**ğŸ‘¤ ì‚¬ìš©ì:** {pair['user']}")
        st.markdown(f"**ğŸ¤– ì–´ì‹œìŠ¤í„´íŠ¸:** {pair['assistant']}")

# ë©”ì¸ ì±— ë©”ì‹œì§€ ì¶œë ¥
for msg in st.session_state.chat_history:
    st.chat_message(msg["role"]).write(msg["content"])

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")
if user_input:
    st.session_state.chat_history.append({"role": "user", "content": user_input})
    memory.chat_memory.add_user_message(user_input)
    st.chat_message("user").write(user_input)

    #rephrase_question()

    # 2. ì‚¬ìš©ì ì…ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±  
    rephrase_input = generate_search_query(st.session_state.chat_history, user_input)
    # st.write(f"ğŸ” ìµœì¢… ê²€ìƒ‰ ì¿¼ë¦¬: {rephrase_input}")
    
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        response = agent_executor.invoke({"input": rephrase_input})
        # 3. LangChain memoryì—ì„œ ìµœì‹  assistant ì‘ë‹µ ê°€ì ¸ì˜¤ê¸°
        assistant_reply = memory.chat_memory.messages[-1].content

        # 4. ì‘ë‹µ ê°€ì ¸ì˜¤ê¸°
        assistant_reply = response.get("output", str(response))
        
        # 5. ì‘ë‹µì„ memoryì™€ stateì— ì¶”ê°€
        memory.chat_memory.add_ai_message(assistant_reply)

        # st.write(f"ğŸ” ë©”ëª¨ë¦¬ ê²°ê³¼: {memory.chat_memory.messages}")
        # st.write(f"ğŸ” history ê²°ê³¼: {st.session_state.chat_history}")
        #ontent = response.get("output", str(response))
        st.session_state.chat_history.append({"role": "assistant", "content": assistant_reply})
        st.chat_message("assistant").write(assistant_reply)
        save_history(session_id, st.session_state.chat_history)