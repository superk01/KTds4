import os
import json
import streamlit as st
from dotenv import load_dotenv
from langchain_community.retrievers import AzureAISearchRetriever
from langchain_openai import AzureChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain.memory import ConversationBufferMemory
from langchain_core.tools import tool
from langchain_community.tools import TavilySearchResults
from langchain.embeddings import AzureOpenAIEmbeddings
from sklearn.metrics.pairwise import cosine_similarity

# ========== í™˜ê²½ ë³€ìˆ˜ ë° ì´ˆê¸° ì„¤ì • ==========
load_dotenv()
HISTORY_DIR = "chat_history"
os.makedirs(HISTORY_DIR, exist_ok=True)

# ========== íˆìŠ¤í† ë¦¬ ê´€ë ¨ í•¨ìˆ˜ ==========
def get_history_path(user_id):
    return os.path.join(HISTORY_DIR, f"{user_id}.json")

def save_history(user_id, history):
    with open(get_history_path(user_id), "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)

def load_history(user_id):
    path = get_history_path(user_id)
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

# ========== ì—°ì† ì§ˆë¬¸ íŒë‹¨ í•¨ìˆ˜ (ì„ë² ë”© ê¸°ë°˜) ==========
embeddings = AzureOpenAIEmbeddings(
    azure_deployment=os.getenv("AZURE_OPENAI_EMBEDDING"),
    openai_api_version="2024-12-01-preview",
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    api_key=os.getenv("OPENAI_API_KEY"),
)

def is_follow_up_question(previous_question: str, current_question: str, threshold: float = 0.75) -> bool:
    prev_vec = embeddings.embed_query(previous_question)
    curr_vec = embeddings.embed_query(current_question)
    sim = cosine_similarity([prev_vec], [curr_vec])[0][0]
    return sim >= threshold

# ========== ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± ==========
def generate_search_query(chat_history, latest_question):
    # ì´ì „ ì§ˆë¬¸ê³¼ ì˜ë¯¸ì ìœ¼ë¡œ ì—°ì†ì¸ì§€ íŒë‹¨
    recent_user_questions = [msg["content"] for msg in chat_history if msg["role"] == "user"]
    if len(recent_user_questions) >= 1:
        previous_question = recent_user_questions[-1]
        if not is_follow_up_question(previous_question, latest_question):
            return latest_question  # ìƒˆë¡œìš´ ì£¼ì œì´ë¯€ë¡œ ë‹¨ë… ì‚¬ìš©

    # ì§ˆë¬¸ + ë‹µë³€ í¬í•¨í•œ ë§¥ë½ìœ¼ë¡œ ì¿¼ë¦¬ ìƒì„±
    combined_context = []
    for msg in chat_history[-4:]:
        combined_context.append(f"{msg['role'].capitalize()}: {msg['content']}")

    prompt = ChatPromptTemplate.from_template(
        "ì´ì „ ëŒ€í™”:\n{context}\n\ní˜„ì¬ ì§ˆë¬¸: {question}\n\n"
        "ìœ„ ëŒ€í™”ë¥¼ ì°¸ê³ í•´ì„œ ê²€ìƒ‰ ì—”ì§„ì— ì í•©í•œ êµ¬ì²´ì ì¸ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•´ì¤˜."
    )
    chain = prompt | llm | StrOutputParser()
    return chain.invoke({"context": "\n".join(combined_context), "question": latest_question})

# ========== ë„êµ¬ ì •ì˜ ==========
@tool
def web_search(query: str) -> str:
    tavily = TavilySearchResults(max_results=3, search_depth="advanced", include_answer=True, include_raw_content=True, include_images=True)
    search_query = generate_search_query(st.session_state.chat_history, query)
    return tavily.invoke(search_query)

@tool
def law_search(query: str) -> str:
    retriever = AzureAISearchRetriever(content_key="chunk", top_k=3, index_name=os.getenv("AZURE_AI_SEARCH_INDEX_NAME_LAW"))
    llm_local = AzureChatOpenAI(deployment_name=os.getenv("AZURE_OPENAI_LLM1"))
    prompt = ChatPromptTemplate.from_template("""Answer the question based only on the context provided.\nContext: {context}\nQuestion: {question}""")
    chain = ({"context": retriever | format_docs, "question": RunnablePassthrough()} | prompt | llm_local | StrOutputParser())
    search_query = generate_search_query(st.session_state.chat_history, query)
    return chain.invoke({"question": search_query})

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# ========== LLM ë° í”„ë¡¬í”„íŠ¸ ==========
llm = AzureChatOpenAI(model=os.getenv("AZURE_OPENAI_LLM2"), temperature=0)

prompt = ChatPromptTemplate.from_messages([
    ("system", "ë‹¹ì‹ ì€ ì§€ëŠ¥í˜• AI Assistantì…ë‹ˆë‹¤. ì´ì „ ì§ˆë¬¸ê³¼ ë§¥ë½ì„ ë°˜ì˜í•´ PDF ë° ì›¹ ê²€ìƒ‰ ë„êµ¬ë¥¼ í†µí•´ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤."),
    ("system", "- 'law_search': PDF ë²•ë¥  ì¸ë±ìŠ¤ ê²€ìƒ‰\n- 'web_search': ìµœì‹  ì •ë³´ë¥¼ Tavilyë¥¼ í†µí•´ ì›¹ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤."),
    ("system", "í•­ìƒ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ì‘ë‹µí•˜ë©°, ë¶ˆëª…í™•í•œ ê²½ìš° í™•ì¸ì„ ìš”ì²­í•©ë‹ˆë‹¤."),
    ("user", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
])

# ========== ì—ì´ì „íŠ¸ ë° ì‹¤í–‰ê¸° ==========
tools = [web_search, law_search]
agent = create_tool_calling_agent(llm, tools, prompt)
memory = ConversationBufferMemory(return_messages=True)
agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory, verbose=True)

# ========== Streamlit UI ==========
st.set_page_config(page_title="Hotel & Web Chat Agent", layout="centered")
st.title("AI Assistant: í˜¸í…” & ì›¹ ê²€ìƒ‰")

user_id = st.text_input("ì‚¬ìš©ì ì´ë¦„ ë˜ëŠ” IDë¥¼ ì…ë ¥í•˜ì„¸ìš”", key="user_id")
if not user_id:
    st.warning("ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•˜ë ¤ë©´ ì‚¬ìš©ì IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    st.stop()

if "chat_history" not in st.session_state:
    st.session_state.chat_history = load_history(user_id)

if st.sidebar.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
    st.session_state.chat_history = []
    save_history(user_id, [])
    st.rerun()

# ì‚¬ì´ë“œë°”ì— ëŒ€í™” íˆìŠ¤í† ë¦¬ ìš”ì•½ í‘œì‹œ
st.sidebar.markdown("## ğŸ’¬ ì´ì „ ëŒ€í™” ë³´ê¸°")
chat_pairs, current_pair = [], {}
for msg in st.session_state.chat_history:
    if msg["role"] == "user":
        if current_pair and "user" in current_pair:
            chat_pairs.append(current_pair)
        current_pair = {"user": msg["content"], "assistant": ""}
    elif msg["role"] == "assistant" and current_pair:
        current_pair["assistant"] = msg["content"]
if current_pair and "user" in current_pair:
    chat_pairs.append(current_pair)

for i, pair in enumerate(chat_pairs):
    with st.sidebar.expander(f"â“ Q{i+1}: {pair['user'][:30]}..."):
        st.markdown(f"**ğŸ‘¤ ì‚¬ìš©ì:** {pair['user']}")
        st.markdown(f"**ğŸ¤– ì–´ì‹œìŠ¤í„´íŠ¸:** {pair['assistant']}")

# ë©”ì¸ ì±— ë©”ì‹œì§€ ì¶œë ¥
for msg in st.session_state.chat_history:
    st.chat_message(msg["role"]).write(msg["content"])

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")
if user_input:
    st.session_state.chat_history.append({"role": "user", "content": user_input})
    st.chat_message("user").write(user_input)
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        response = agent_executor.invoke({"input": user_input})
        content = response.get("output", str(response))
        st.session_state.chat_history.append({"role": "assistant", "content": content})
        st.chat_message("assistant").write(content)
        save_history(user_id, st.session_state.chat_history)
