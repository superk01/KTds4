import os
from langchain_community.retrievers import AzureAISearchRetriever
from langchain_openai import AzureChatOpenAI
from dotenv import load_dotenv
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_community.tools import TavilySearchResults
from langchain.agents import create_tool_calling_agent
from langchain.agents import AgentExecutor
from langchain_core.tools import tool
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.memory import ConversationBufferMemory
import streamlit as st
import json

load_dotenv()

#from langchain.agents import AgentExecutor

# ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
HISTORY_DIR = "chat_history"
os.makedirs(HISTORY_DIR, exist_ok=True)

# ğŸ”¹ íˆìŠ¤í† ë¦¬ ì €ì¥ ë° ë¶ˆëŸ¬ì˜¤ê¸°
def get_history_path(user_id):
    return os.path.join(HISTORY_DIR, f"{user_id}.json")

def save_history(user_id, history):
    with open(get_history_path(user_id), "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)

def load_history(user_id):
    path = get_history_path(user_id)
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

def generate_search_query(chat_history, latest_question):
    # ì§ì „ ì‚¬ìš©ì ë©”ì‹œì§€ë§Œ ê°€ì ¸ì˜¤ê¸° (ë˜ëŠ” ì „ì²´ ëŒ€í™” ìš”ì•½ë„ ê°€ëŠ¥)
    context = [msg["content"] for msg in chat_history if msg["role"] == "user"][-2:]

    prompt = ChatPromptTemplate.from_template(
        "ì´ì „ ì§ˆë¬¸ë“¤: {context}\n\ní˜„ì¬ ì§ˆë¬¸: {question}\n\nìœ„ ë§¥ë½ì„ ë°˜ì˜í•´ì„œ ê²€ìƒ‰ ì—”ì§„ì— ì ì ˆí•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•´ì¤˜."
    )

    chain = prompt | llm | StrOutputParser()

    result = chain.invoke({
        "context": "\n".join(context),
        "question": latest_question
    })

    return result

user_id = st.text_input("ì‚¬ìš©ì ì´ë¦„ ë˜ëŠ” IDë¥¼ ì…ë ¥í•˜ì„¸ìš”", key="user_id")

if not user_id:
    st.warning("ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•˜ë ¤ë©´ ì‚¬ìš©ì IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    st.stop()

@tool
def web_search(query: str) -> str:   
    """Search the web using Tavily."""

    tavilyRetriever = TavilySearchResults(
        max_results=3,  # ë°˜í™˜í•  ê²°ê³¼ì˜ ìˆ˜
        search_depth="advanced",  # ê²€ìƒ‰ ê¹Šì´: basic ë˜ëŠ” advanced
        include_answer=True,  # ê²°ê³¼ì— ì§ì ‘ì ì¸ ë‹µë³€ í¬í•¨
        include_raw_content=True,  # í˜ì´ì§€ì˜ ì›ì‹œ ì½˜í…ì¸  í¬í•¨
        include_images=True,  # ê²°ê³¼ì— ì´ë¯¸ì§€ í¬í•¨
        # include_domains=[...],  # íŠ¹ì • ë„ë©”ì¸ìœ¼ë¡œ ê²€ìƒ‰ ì œí•œ
        # exclude_domains=[...],  # íŠ¹ì • ë„ë©”ì¸ ì œì™¸
        # name="...",  # ê¸°ë³¸ ë„êµ¬ ì´ë¦„ ë®ì–´ì“°ê¸°
        # description="...",  # ê¸°ë³¸ ë„êµ¬ ì„¤ëª… ë®ì–´ì“°ê¸°
        # args_schema=...,  # ê¸°ë³¸ args_schema ë®ì–´ì“°ê¸°
    )

    # ì´ì „ ëŒ€í™” ì´ë ¥ ë¡œê¹…
    print(json.dumps(st.session_state.chat_history, ensure_ascii=False, indent=2))
    
    # ìµœì‹  ëŒ€í™”ì™€ ì—°ê²° (í•„ìš”ì‹œ chat_history ë¥¼ ë°›ë„ë¡ ìˆ˜ì •)
    query_to_search = generate_search_query(
        st.session_state.chat_history,  # ëŒ€í™” ì´ë ¥
        query                           # í˜„ì¬ ì§ˆë¬¸
    )
    
    return tavilyRetriever.invoke(query_to_search)

@tool
def hotel_search(query: str) -> str:
    """Search for Hotel information in PDF files."""
    retriever = AzureAISearchRetriever(
        content_key="chunk",      # ì¸ë±ìŠ¤ ë‚´ ì»¨í…ì¸  í•„ë“œëª…
        top_k=3,                   # ë°˜í™˜í•  ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜
        index_name=os.getenv("AZURE_AI_SEARCH_INDEX_NAME"),  # Azure Search ì¸ë±ìŠ¤ ì´ë¦„
    )

    llm = AzureChatOpenAI(
        deployment_name=os.getenv("AZURE_OPENAI_LLM1"),  # í™˜ê²½ ë³€ìˆ˜ëª… í™•ì¸
    )

    prompt = ChatPromptTemplate.from_template(
        """Answer the question based only on the context provided.
        Context: {context}
        Question: {question}"""
    )

    chain = (
        {
            "context": retriever | format_docs,
            "question": RunnablePassthrough()
        }
        | prompt
        | llm
        | StrOutputParser()
    )

    # ìµœì‹  ëŒ€í™”ì™€ ì—°ê²° (í•„ìš”ì‹œ chat_history ë¥¼ ë°›ë„ë¡ ìˆ˜ì •)
    query_to_search = generate_search_query(
        st.session_state.chat_history,  # ëŒ€í™” ì´ë ¥
        query                           # í˜„ì¬ ì§ˆë¬¸
    )
    # ì²´ì¸ ì‹¤í–‰
    result = chain.invoke({"question": query_to_search})
    return result

@tool
def law_search(query: str) -> str:
    """Search for Law information in PDF files."""
    retriever = AzureAISearchRetriever(
        content_key="chunk",      # ì¸ë±ìŠ¤ ë‚´ ì»¨í…ì¸  í•„ë“œëª…
        top_k=3,                   # ë°˜í™˜í•  ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜
        index_name=os.getenv("AZURE_AI_SEARCH_INDEX_NAME_LAW"),  # Azure Search ì¸ë±ìŠ¤ ì´ë¦„
    )

    llm = AzureChatOpenAI(
        deployment_name=os.getenv("AZURE_OPENAI_LLM1"),  # í™˜ê²½ ë³€ìˆ˜ëª… í™•ì¸
    )

    prompt = ChatPromptTemplate.from_template(
        """Answer the question based only on the context provided.
        Context: {context}
        Question: {question}"""
    )

    chain = (
        {
            "context": retriever | format_docs,
            "question": RunnablePassthrough()
        }
        | prompt
        | llm
        | StrOutputParser()
    )

    # ì´ì „ ëŒ€í™” ì´ë ¥ ë¡œê¹…
    print(json.dumps(st.session_state.chat_history, ensure_ascii=False, indent=2))

    # ìµœì‹  ëŒ€í™”ì™€ ì—°ê²° (í•„ìš”ì‹œ chat_history ë¥¼ ë°›ë„ë¡ ìˆ˜ì •)
    query_to_search = generate_search_query(
        st.session_state.chat_history,  # ëŒ€í™” ì´ë ¥
        query                           # í˜„ì¬ ì§ˆë¬¸
    )

    # ì²´ì¸ ì‹¤í–‰
    result = chain.invoke({"question": query_to_search})
    return result

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


#tools = [web_search, hotel_search]  # ì‚¬ìš©í•  ë„êµ¬ ëª©ë¡
tools = [web_search, law_search]  # ì‚¬ìš©í•  ë„êµ¬ ëª©ë¡


llm = AzureChatOpenAI(model=os.getenv("AZURE_OPENAI_LLM2"),temperature=0)

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
# prompt = ChatPromptTemplate.from_messages([
#     ("system", "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” AI Assistantì…ë‹ˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œëŠ” í˜¸í…” ì •ë³´ ê²€ìƒ‰ì„ ë‹´ë‹¹í•˜ë©°, ì›¹ ê²€ìƒ‰ ë„êµ¬ë¥¼ í†µí•´ ì¶”ê°€ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."),
#     ("system", "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¨¼ì € ì˜ì–´ë¡œ ë²ˆì—­í•˜ê³  ì´í•´í•©ë‹ˆë‹¤."),    
#     ("system", "ì§ˆë¬¸ì— ë”°ë¼ ë‘ ê°€ì§€ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: í˜¸í…” ì •ë³´ ê²€ìƒ‰ê³¼ ì›¹ ê²€ìƒ‰."),
#     ("system", "í˜¸í…” ì •ë³´ ê²€ìƒ‰ì€ PDF íŒŒì¼ì—ì„œ í˜¸í…” ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ê³ , ì›¹ ê²€ìƒ‰ì€ Tavilyë¥¼ í†µí•´ ì›¹ì—ì„œ ì •ë³´ë¥¼ ì°¾ìŠµë‹ˆë‹¤."),
#     ("user", "{input}"),
#     MessagesPlaceholder(variable_name="agent_scratchpad")
# ])

prompt = ChatPromptTemplate.from_messages([
    ("system", 
     "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ì§€ëŠ¥í˜• AI Assistantì…ë‹ˆë‹¤. "
     "ê¸°ë³¸ì ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì€ PDF íŒŒì¼ì—ì„œ ê²€ìƒ‰í•˜ë©°, í•„ìš”í•  ê²½ìš° ì›¹ ê²€ìƒ‰ ë„êµ¬ë¥¼ í†µí•´ ìµœì‹  ì •ë³´ë¥¼ ë³´ì™„í•©ë‹ˆë‹¤."),

    # ("system", 
    #  "ì…ë ¥ëœ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ í•œê¸€ì¼ ê²½ìš°, ë‚´ë¶€ì ìœ¼ë¡œ ì˜ì–´ë¡œ ë²ˆì—­í•˜ì—¬ ì²˜ë¦¬í•œ í›„ ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•´ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."),
    ("system","ì§ˆë¬¸ì„ ë°›ì€ í›„ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ì´ì „ ì§ˆë¬¸ê³¼ ì—°ê´€ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , ì´ì „ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ê°€ ìˆë‹¤ë©´ ê·¸ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."),

    ("system", 
     "ì‚¬ìš©ìì˜ ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ë‹¤ìŒ ë‘ ê°€ì§€ ë„êµ¬ ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n"
     "- 'law_search': Azure Cognitive Searchë¥¼ í†µí•´ ì‚¬ì „ì— ì¸ë±ì‹±ëœ PDFì—ì„œ ë²•ë¥  ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n"
     "- 'web_search': law_searchë¡œ ì°¾ì§€ ëª»í•œ ë‚´ìš©ì„ Tavily ê²€ìƒ‰ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ì—ì„œ ìµœì‹  ì •ë³´ë‚˜ ì¶”ê°€ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."),

    ("system", 
     "ëª¨ë“  ì‘ë‹µì€ ì¹œì ˆí•˜ê³ , ëª…í™•í•˜ë©°, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ì •ë³´ë§Œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. "
     "ë¶ˆí•„ìš”í•œ ì¶”ì¸¡ì€ í”¼í•˜ê³ , ëª…í™•í•˜ì§€ ì•Šì€ ê²½ìš° í™•ì¸ì„ ìš”ì²­í•˜ì„¸ìš”."),

    ("user", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
])


# ì—ì´ì „íŠ¸ ìƒì„± (ë„êµ¬ í˜¸ì¶œ)
agent = create_tool_calling_agent(llm, tools, prompt)

memory = ConversationBufferMemory(return_messages=True)

# ì—ì´ì „íŠ¸ ì‹¤í–‰ê¸° ìƒì„±
agent_executor = AgentExecutor(
    agent=agent,      # ë„êµ¬ í˜¸ì¶œ ì—ì´ì „íŠ¸
    tools=tools,      # ë„êµ¬ ëª©ë¡
    memory=memory,    # ëŒ€í™” ë©”ëª¨ë¦¬
    verbose=True      # ìƒì„¸ ë¡œê·¸ ì¶œë ¥
    )

# Streamlit UI ì„¤ì •
st.set_page_config(page_title="Hotel & Web Chat Agent", layout="centered")
st.title("AI Assistant: í˜¸í…” & ì›¹ ê²€ìƒ‰")

# ğŸ”¹ ëŒ€í™” ì´ˆê¸°í™”
if st.sidebar.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
    st.session_state.chat_history = []
    save_history(user_id, [])
    st.rerun()

if "chat_history" not in st.session_state:
#    st.session_state.chat_history = []
    st.session_state.chat_history = save_history(user_id)

# ğŸ”¹ ì‚¬ì´ë“œë°” ëŒ€í™” ë¦¬ìŠ¤íŠ¸ UI
st.sidebar.markdown("## ğŸ’¬ ì´ì „ ëŒ€í™” ë³´ê¸°")

# ëŒ€í™”ë¥¼ ì§ˆë¬¸-ë‹µë³€ ë‹¨ìœ„ë¡œ ê·¸ë£¹í•‘
chat_pairs = []
current_pair = {}
for msg in st.session_state.chat_history:
    if msg["role"] == "user":
        if current_pair and "user" in current_pair:
            chat_pairs.append(current_pair)
        current_pair = {"user": msg["content"], "assistant": ""}
    elif msg["role"] == "assistant" and current_pair:
        current_pair["assistant"] = msg["content"]
# ë§ˆì§€ë§‰ ì§ˆë¬¸-ë‹µë³€ ìŒ ì¶”ê°€
if current_pair and "user" in current_pair:        
    chat_pairs.append(current_pair)

# ì‚¬ì´ë“œë°”ì— ìš”ì•½ í‘œì‹œ
for i, pair in enumerate(chat_pairs):
    with st.sidebar.expander(f"â“ Q{i+1}: {pair['user'][:30]}..."):
        st.markdown(f"**ğŸ‘¤ ì‚¬ìš©ì:** {pair['user']}")
        st.markdown(f"**ğŸ¤– ì–´ì‹œìŠ¤í„´íŠ¸:** {pair['assistant']}")

# ì±— ë©”ì‹œì§€ ì¶œë ¥
for msg in st.session_state.chat_history:
    st.chat_message(msg["role"]).write(msg["content"])

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")
if user_input:
    st.session_state.chat_history.append({"role": "user", "content": user_input})
    st.chat_message("user").write(user_input)
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
#        response = agent_executor.invoke({"input": user_input, "chat_history": st.session_state.chat_history})
        response = agent_executor.invoke({"input": user_input})
        content = response.get("output", str(response))
        st.session_state.chat_history.append({"role": "assistant", "content": content})
        st.chat_message("assistant").write(content)
        load_history(user_id)



